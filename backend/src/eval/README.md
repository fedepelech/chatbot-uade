# Evaluaci√≥n de M√©tricas para RAG (Retrieval-Augmented Generation)

Este documento explica las m√©tricas implementadas para evaluar el sistema RAG del chatbot, c√≥mo funcionan, sus diferencias y los resultados obtenidos.

## üìä M√©tricas Implementadas

### 1. BERTScore (F1: 0.80)

**¬øQu√© es?**
BERTScore es una m√©trica basada en embeddings contextuales que eval√∫a la similitud sem√°ntica entre el texto de referencia y la predicci√≥n.

**C√≥mo funciona:**
- Utiliza el modelo BERT para generar representaciones vectoriales de ambas oraciones
- Calcula la similitud de palabras entre ambas representaciones mediante la similitud de coseno
- Retorna tres m√©tricas:
  - **Precision** (0.73): Proporci√≥n de tokens predichos que est√°n en la referencia
  - **Recall** (0.89): Proporci√≥n de tokens de referencia capturados en la predicci√≥n
  - **F1** (0.80): Media arm√≥nica de precision y recall

**Ventajas:**
- Captura similitud sem√°ntica, no solo lexical
- Funciona bien con parafraseo
- M√°s robusto que m√©tricas n-gram

### 2. ROUGE (Rouge-L: 0.48)

**¬øQu√© es?**
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) es una familia de m√©tricas orientadas a la coherencia de n-gramas.

**Variantes implementadas:**
- **ROUGE-1** (0.50): Coincidencia de unigramas (palabras individuales)
- **ROUGE-2** (0.42): Coincidencia de bigramas (pares consecutivos)
- **ROUGE-L** (0.48): Mayor subsecuencia com√∫n (LCS) que eval√∫a fluidez

**C√≥mo funciona:**
- Cuenta cu√°ntos n-gramas del texto candidato aparecen en el texto de referencia
- Considera la longitud de la secuencia m√°s larga com√∫n
- Prioriza el recall sobre precision (orientado a gist/captaci√≥n)

**Cu√°ndo usar:**
- √ötil para tareas de resumen y generaci√≥n de texto
- Eval√∫a cobertura de informaci√≥n clave

### 3. BLEU (BLEU-4: 0.24)

**¬øQu√© es?**
BLEU (Bilingual Evaluation Understudy) es una m√©trica cl√°sica para evaluaci√≥n autom√°tica de traducci√≥n y generaci√≥n.

**Variantes implementadas:**
- **BLEU-1** (0.32): Coincidencia de unigramas
- **BLEU-2** (0.29): Coincidencia de bigramas
- **BLEU-4** (0.24): Coincidencia de 4-gramas (la m√°s estricta)

**C√≥mo funciona:**
- Cuenta n-gramas que coinciden exactamente entre referencia y predicci√≥n
- Aplica un factor de penalizaci√≥n por brevedad (brevity penalty)
- Usa suavizado (smoothing) para evitar cero cuando no hay coincidencias

**Caracter√≠sticas:**
- Estricto: Requiere coincidencia exacta de n-gramas
- Penaliza respuestas demasiado cortas o largas
- Menos tolerante al parafraseo que ROUGE

### 4. Cosine Similarity (0.80)

**¬øQu√© es?**
Similitud de coseno entre embeddings sem√°nticos de oraciones completas.

**C√≥mo funciona:**
- Usa el modelo `sentence-transformers` para generar embeddings densos
- Calcula la similitud del coseno entre vectores de referencia y predicci√≥n
- Valor entre -1 y 1 (nuestro resultado: 0.80 = 80% similar)

**Ventajas:**
- Captura similitud sem√°ntica a nivel de oraci√≥n completa
- Eficiente computacionalmente
- Independiente de la estructura sint√°ctica

## üîç Diferencias entre M√©tricas

| Caracter√≠stica | BERTScore | ROUGE | BLEU | Cosine Similarity |
|---|---|---|---|---|
| **Enfoque** | Sem√°ntico contextual | N-gramas recall | N-gramas precision | Sem√°ntico vectorial |
| **Tolerancia a parafraseo** | Alta | Media | Baja | Alta |
| **Orden de palabras** | No cr√≠tico | Parcial | Cr√≠tico | No cr√≠tico |
| **Mejor para** | Significado sem√°ntico | Cobertura de info | Coincidencia exacta | Similitud global |

## üìà An√°lisis de Resultados

### Resumen de M√©tricas

```json
{
  "bertscore": { "f1": 0.80, "precision": 0.73, "recall": 0.89 },
  "rouge": { "rouge1": 0.50, "rouge2": 0.42, "rougeL": 0.48 },
  "bleu": { "bleu1": 0.32, "bleu2": 0.29, "bleu4": 0.24 },
  "cosine_similarity": { "average": 0.80 }
}
```

### Interpretaci√≥n

**Fortalezas del sistema:**
1. **BERTScore F1: 0.80** - Excelente similitud sem√°ntica, indica que el chatbot comprende bien el significado
2. **Recall alto (0.89)** - El sistema captura la mayor√≠a de informaci√≥n relevante de la referencia
3. **Cosine Similarity: 0.80** - Respuestas tienen alta similitud sem√°ntica con las respuestas esperadas

**√Åreas de mejora:**
1. **BLEU bajo** - Las respuestas tienden a ser m√°s largas y parafraseadas que la referencia exacta
2. **ROUGE moderado** - Hay contenido adicional en las respuestas que no est√° en la referencia

**Patr√≥n observado:**
Las predicciones del chatbot tienen tendencia a:
- Incluir frases de cortes√≠a y cierre ("¬°√âxitos!", "No dudes en preguntar")
- Parafrasear la informaci√≥n en lugar de replicar el texto exacto
- Proporcionar contexto adicional o sugerencias

Esto es **comportamiento esperado** en un chatbot conversacional, que busca generar respuestas naturales y √∫tiles, no textos id√©nticos a las referencias.

## üõ†Ô∏è Implementaci√≥n T√©cnica

### Arquitectura

El sistema de evaluaci√≥n consta de dos componentes principales:

#### 1. Generaci√≥n de Predicciones (`rag_eval_cli.ts`)

- **TipoScript/Node.js** - CLI para generar predicciones
- Carga dataset de preguntas y respuestas de referencia
- Inicializa el pipeline RAG con Ollama
- Ejecuta cada pregunta contra el sistema RAG
- Guarda resultados en `predictions.json` con estructura:
  ```typescript
  {
    index: number,
    question: string,
    reference: string,  // Respuesta esperada
    prediction: string  // Respuesta del chatbot
  }
  ```

#### 2. C√°lculo de M√©tricas (`metrics.py`)

- **Python** - Script para calcular m√©tricas de evaluaci√≥n
- Procesa `predictions.json` y calcula 4 tipos de m√©tricas
- Utiliza bibliotecas especializadas para cada m√©trica:
  - `bert-score`: BERTScore
  - `rouge-score`: ROUGE
  - `nltk`: BLEU
  - `sentence-transformers` + `scikit-learn`: Cosine Similarity
- Genera `report.json` con m√©tricas agregadas y por-item

### Flujo de Evaluaci√≥n

```
[Dataset JSON] ‚Üí [rag_eval_cli.ts] ‚Üí [predictions.json]
                                           ‚Üì
                                   [metrics.py]
                                           ‚Üì
                                     [report.json]
```

### Uso

**1. Generar predicciones:**
```bash
npm run eval:rag
# o
tsx backend/src/eval/rag_eval_cli.ts --dataset data/generic-faqs.json --out backend/src/eval/predictions.json
```

**2. Calcular m√©tricas:**
```bash
python backend/src/eval/metrics.py \
  --predictions backend/src/eval/predictions.json \
  --out backend/src/eval/report.json \
  --lang es
```

### Dependencias

**Node.js:**
- Sistema RAG (LangChain)
- Ollama LLM

**Python:**
```txt
bert-score==0.3.13          # BERTScore
rouge-score==0.1.2          # ROUGE
nltk==3.9.1                 # BLEU + tokenizaci√≥n
sentence-transformers==3.1.1 # Embeddings
scikit-learn==1.5.2         # Cosine similarity
```

## üí° Conclusiones

El sistema RAG muestra **buen rendimiento sem√°ntico** (BERTScore 0.80, Cosine 0.80) pero genera respuestas m√°s extensas y conversacionales que las referencias. Esto es apropiado para un chatbot, que debe:
- Capturar el significado correcto ‚úÖ
- Proporcionar contexto adicional ‚úÖ
- Mantener tono conversacional ‚úÖ

Las m√©tricas basadas en n-gramas (BLEU, ROUGE) son √∫tiles para evaluar coincidencia textual, pero BERTScore y Cosine Similarity son m√°s apropiadas para evaluar chatbots conversacionales.

